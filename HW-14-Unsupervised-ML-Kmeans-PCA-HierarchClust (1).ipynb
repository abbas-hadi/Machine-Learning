{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Overview\n",
    "`\n",
    "By   : Dr. James G. Shanahan\n",
    "EMAIL: James.Shanahan AT Gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For HW14, you will be expected to answer questions about your work in this notebook via Canvas (Modules->Module 14->HW14 Notebook and submission form). You may wish to reference this quiz while working through the assignment.__\n",
    "## Submission instructionsÂ¶\n",
    "Before completing this homework,\n",
    "1. please review this homework's submission form on Canvas available under the \"Modules\" menu option and briefly review this notebook end to end.\n",
    "2. To get you started we provide a template solution with missing code and prompts. Please complete the missing code, run the experiments and log your results.\n",
    "3. When you're sufficiently happy with your results, please begin the submission process on Canvas. Use the submission form for this homework available under \"Modules\" menu option. Please note that the submission form is available at the same place where you downloaded the homework from.\n",
    "4. You may wish to reference the submission form (Modules->Module 14->HW14 Notebook and submission form) while working through the tasks.\n",
    "\n",
    "The goals of this HW include the following:\n",
    "* Evaluate unlabeled data using a dendrogram\n",
    "* Conduct hierarchical clustering in a pipeline\n",
    "* Extend a homegrown implementation of KMeans [optional]\n",
    "* Visualize cluster boundaries using KMeans   \n",
    "* Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Perform in code (check lab notebook) or by hand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\" alt=\"Drawing\" style=\"width: =600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 2(a)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarityMatrix = np.array([.3,.4,.7,.5,.8,.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b2d0c4df2b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def augmented_dendrogram(*args, **kwargs):\n",
    "\n",
    "    ddata = hierarchy.dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            plt.plot(x, y, 'ro')\n",
    "            plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -8),\n",
    "                         textcoords='offset points',\n",
    "                         va='top', ha='center')\n",
    "        plt.title('Dendrogram with Complete Linkage')\n",
    "    return ddata\n",
    "# Use the dissimilarityMatrix to create the clustering matrix\n",
    "# Make sure that the method chosen is 'complete'\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "z = hierarchy.linkage(X, 'complete')\n",
    "\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "dn = augmented_dendrogram(z, labels=['1','2','3','4'], \n",
    "                         leaf_rotation=0,  \n",
    "                        leaf_font_size=8.,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 2(b)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-55bc6c18775f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'single'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#==================================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#               Your code ends here                #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def augmented_dendrogram(*args, **kwargs):\n",
    "\n",
    "    ddata = hierarchy.dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            plt.plot(x, y, 'ro')\n",
    "            plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -8),\n",
    "                         textcoords='offset points',\n",
    "                         va='top', ha='center')\n",
    "        plt.title('Dendrogram with Single Linkage')\n",
    "    return ddata\n",
    "\n",
    "# Use the dissimilarityMatrix to create the clustering matrix\n",
    "# Make sure that the method chosen is 'single'\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "z = hierarchy.linkage(X, 'single')\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "dn = augmented_dendrogram(z, labels=['1','2','3','4'],\n",
    "                         leaf_rotation=0,  # rotates the x axis labels\n",
    "                        leaf_font_size=8.,  # font size for the x axis labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 2(c)__\n",
    "\n",
    "Points in each cluster based on first dendrogram:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 2(d)__\n",
    "\n",
    "Points in each cluster based on second dendrogram:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 2(e)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarityMatrix = np.array([.3,.4,.7,.5,.8,.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dendrogram(*args, **kwargs):\n",
    "\n",
    "    ddata = hierarchy.dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            plt.plot(x, y, 'ro')\n",
    "            plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -8),\n",
    "                         textcoords='offset points',\n",
    "                         va='top', ha='center')\n",
    "        plt.title('Dendrogram with Complete Linkage')\n",
    "    return ddata\n",
    "\n",
    "z = hierarchy.linkage(dissimilarityMatrix, method='complete')\n",
    "print (z)\n",
    "# Reorder clusters in order to reposition the leaves\n",
    "# But keep the same meaning of the dendogram as in question 2(a)\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "z2 = np.vstack((z[..],z[..],z[..]))\n",
    "\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "\n",
    "dn = augmented_dendrogram(z2, labels=['1','2','3','4'], \n",
    "                         leaf_rotation=0,  \n",
    "                        leaf_font_size=8.,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering  \n",
    "\n",
    "<img src=\"image1.png\" alt=\"Drawing\" style=\"width: =600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 3(a)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/USArrests.csv')\n",
    "df.columns = ['States', 'Murder', 'Assault', 'UrbanPop', 'Rape']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image 4.png\" alt=\"Drawing\" style=\"width: =600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical axis of the dendrogram represents the distance or dissimilarity between clusters. The horizontal axis\n",
    "represents the objects and clusters. The dendrogram is simple to interpret.Each joining of two clusters is represented on the graph by the splitting of\n",
    "a vertical line into two vertical lines. The vertical position of the split, shown by the short horizontal bar,\n",
    "gives the distance (dissimilarity) between the two clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels from data for each observation\n",
    "names = df['States'].values\n",
    "observations = df.iloc[:,1:].values\n",
    "\n",
    "# create clustering matrix using \"complete\" linkage and Euclidean distance\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "linkage = hierarchy.linkage(name)\n",
    "\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "# plot a dendrogram of the hierarchical clustering\n",
    "plt.figure(figsize=(16,6))\n",
    "dd = augmented_dendrogram(linkage, leaf_rotation=90, labels=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 3(b)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 clusters based on observations data\n",
    "# using \"complete\" linkage and Euclidean distance\n",
    "# and predict the states that belong to each cluster\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "ac = AgglomerativeClustering(...)\n",
    "preds = ac.fit_predict(...)\n",
    "\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "\n",
    "# identify the indices corresponding to each cluster\n",
    "clusters = []\n",
    "for n in set(preds):\n",
    "    clusters.append(np.where(preds==n)[0])\n",
    "\n",
    "# display state names corresponding to the cluster indices\n",
    "for n in range(3):\n",
    "    print('Cluster {}: {}\\n'.format(n+1, names[clusters[n]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 3(c)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create a pipeline that uses StandardScaler to standardize the data\n",
    "# Then uses AgglomerativeClustering to generate 3 clusters based on observations data\n",
    "# using \"complete\" linkage and Euclidean distance\n",
    "# and predict the states that belong to each cluster\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "\n",
    "pipeline = make_pipeline(..., ...)\n",
    "preds_std=pipeline.fit_predict(...)\n",
    "\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "clusters_std = []\n",
    "for n in set(preds_std):\n",
    "    clusters_std.append(np.where(preds_std==n)[0])\n",
    "# display state names corresponding to the cluster indices\n",
    "for n in range(3):\n",
    "    print('Cluster {}: {}\\n'.format(n+1, names[clusters_std[n]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Question 3(d)__\n",
    "\n",
    "What effect did scaling have (compare scaled output in part c to original output in part b above). In your opinion should the variables be scaled before the inter-observation dissimilarities are computed? provide a justification for your answer. You can use the results of the silhouette scores obtained in 3.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette coefficient\n",
    "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n",
    "\n",
    "* a: The mean distance between each sample point and all other points in the same class.\n",
    "* b: The mean distance between each sample  point and all other points in the next nearest cluster.\n",
    "The Silhouette Coefficient s for a single sample is then given as:  \n",
    "$ s = \\frac{b - a}{max(a, b)} $  \n",
    "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\n",
    "* __Example__:  \n",
    "The following example shows how the silhouette score is calculated for different clustering scenarios. You can see that in the second case, when the points of the first cluster are closer to each other and far from the point in cluster 2 the silhouette score increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy, pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_points= np.array([[1,0],[1,0],[1,2],[2,2],[3,1],[3,3]])\n",
    "clusters= [1,1,2,2,3,3]\n",
    "# Create figure\n",
    "fig, ((ax1, ax2)) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.scatter(example_points[:,0],example_points[:,1],c=clusters)\n",
    "ax1.set(title=f\"Cluster example 1\\nclusters are not compact\\nsilhouette_score: {np.round(silhouette_score(example_points, clusters, metric='euclidean'),3)}\")\n",
    "ax1.grid()\n",
    "\n",
    "print(\"The silhouette score is : \",np.round(silhouette_score(example_points, clusters, metric='euclidean'),3))\n",
    "example_points= np.array([[1,0],[1.1,0.1],[1.2,0.2],[0.9,0],[1,0.2],[5,5]])\n",
    "clusters= [1,1,1,1,1,2]\n",
    "print(\"The silhouette score is : \",np.round(silhouette_score(example_points, clusters, metric='euclidean'),3))\n",
    "\n",
    "ax2.scatter(example_points[:,0],example_points[:,1],c=clusters)\n",
    "ax2.grid()\n",
    "ax2.set(title=f\"Cluster example 2\\nCompact clusters that are far apart\\nsilhouette_score: {np.round(silhouette_score(example_points, clusters, metric='euclidean'),3)}\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The silhouette score of the first clustering (no standardization) is : \",np.round(silhouette_score(observations, preds, metric='euclidean'),3))\n",
    "print(\"The silhouette score of the second clustering (standardized data) is : \",np.round(silhouette_score(observations, preds_std, metric='euclidean'),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + Logistic Regression on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains 70,000 handwritten digits (60,000 training images and 10,000 test images).  Each has been size-normalized and centered in a fixed-size image. The originial dataset and additional information is hosted <a href='http://yann.lecun.com/exdb/mnist/'>here</a>, but it requires some effort to decompress and load.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating image data using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST from openml.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "#Normalize the image intensity data\n",
    "X = X / 255.\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "np.random.seed(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the train test split is 80% training and 20% test. \n",
    "In this case, we will use 6/7th of the data to be training and 1/7th of the data to be in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, test_img, train_lbl, test_lbl = train_test_split(X_train, y_train, test_size=1/7.0, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img=train_img.to_numpy()\n",
    "test_img=test_img.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select enough components to account for .95 of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(.95)\n",
    "pca_train_img = pca.fit_transform(train_img)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore image using pca transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximation = pca.inverse_transform(pca_train_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original image to an approximation based on 95% of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,4));\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.imshow(train_img[0].reshape(28,28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('784 components', fontsize = 12)\n",
    "plt.title('Original Image', fontsize = 14);\n",
    "\n",
    "# 154 principal components\n",
    "plt.subplot(1, 2, 2);\n",
    "plt.imshow(approximation[0].reshape(28, 28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('154 components', fontsize = 12)\n",
    "plt.title('95% of Explained Variance', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If n_components is not set all components are kept (784 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(train_img)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing explained variance\n",
    "tot = sum(pca.explained_variance_)\n",
    "print('Total explained variance:',tot)\n",
    "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_, reverse=True)] \n",
    "print('Percentage of explained variance associated with top 5 components:\\n',var_exp[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative explained variance\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot can help you understand the level of redundancy present in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.step(range(1, 785), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.title('Cumulative Explained Variance as a Function of the Number of Components')\n",
    "plt.ylabel('Cumulative Explained variance')\n",
    "plt.xlabel('Principal components')\n",
    "plt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\n",
    "plt.axhline(y = 90, color='c', linestyle='--', label = '90% Explained Variance')\n",
    "plt.axhline(y = 85, color='r', linestyle='--', label = '85% Explained Variance')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Principal Components for 99%, 95%, 90%, and 85% of Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices corresponding to the first occurrence are returned with the np.argmax function\n",
    "# Adding 1 to the end of value in list as principal components start from 1 and indexes start from 0 (np.argmax)\n",
    "componentsVariance = [784, np.argmax(cum_var_exp > 99) + 1, np.argmax(cum_var_exp > 95) + 1, np.argmax(cum_var_exp > 90) + 1, np.argmax(cum_var_exp >= 85) + 1]\n",
    "componentsVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainedVariance(percentage, images): \n",
    "    # percentage should be a decimal from 0 to 1 \n",
    "    pca = PCA(percentage)\n",
    "    pca.fit(images)\n",
    "    components = pca.transform(images)\n",
    "    approxOriginal = pca.inverse_transform(components)\n",
    "    return approxOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4));\n",
    "\n",
    "# Original Image (784 components)\n",
    "plt.subplot(1, 5, 1);\n",
    "plt.imshow(train_img[5].reshape(28,28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('784 Components', fontsize = 12)\n",
    "plt.title('Original Image', fontsize = 14);\n",
    "\n",
    "# 331 principal components\n",
    "plt.subplot(1, 5, 2);\n",
    "plt.imshow(explainedVariance(.99, train_img)[5].reshape(28, 28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('331 Components', fontsize = 12)\n",
    "plt.title('99% of Explained Variance', fontsize = 14);\n",
    "\n",
    "# 154 principal components\n",
    "plt.subplot(1, 5, 3);\n",
    "plt.imshow(explainedVariance(.95, train_img)[5].reshape(28, 28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('154 Components', fontsize = 12)\n",
    "plt.title('95% of Explained Variance', fontsize = 14);\n",
    "\n",
    "# 87 principal components\n",
    "plt.subplot(1, 5, 4);\n",
    "plt.imshow(explainedVariance(.90, train_img)[5].reshape(28, 28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('87 Components', fontsize = 12)\n",
    "plt.title('90% of Explained Variance', fontsize = 14);\n",
    "\n",
    "# 59 principal components\n",
    "plt.subplot(1, 5, 5);\n",
    "plt.imshow(explainedVariance(.85, train_img)[5].reshape(28, 28)*255,\n",
    "              cmap = plt.cm.gray, interpolation='nearest',\n",
    "              clim=(0, 255));\n",
    "plt.xlabel('59 Components', fontsize = 12)\n",
    "plt.title('85% of Explained Variance', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __TASK__ : logistic regression  model using 40 components\n",
    "Report the test accuracy of a logistic regression  model trained using 40 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a pipeline that instatnciates a logistic regression model in 'sag' mode with max_iter = 1000\n",
    "# using 40 PCA components\n",
    "# and fit the train_img and train_lbl\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "pipeline = make_pipeline(..., ...)\n",
    "pipeline.fit(...,...)\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "preds = pipeline.predict(test_img)\n",
    "acc = accuracy_score(test_lbl, preds)\n",
    "print('Accuracy of logistic regression with 40 PCA components:', np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __TASK__ : Approximation of original data\n",
    "PCA can be used to compress high dimensional data to lower dimensional data. PCA can also take the compressed representation of the data (lower dimensional data) back to an approximation of the original high dimensional data. \n",
    "* Select a number from __from the MNIST test data generated with the split above__. \n",
    "* Show a visualization of the chosen example (original image) __and__ the same image in an approximated form using the first 40 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(imageA, imageB):\n",
    "    # the 'Root Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return np.sqrt(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 40 pca componenents and fit the train_img data and use random state as 12\n",
    "# transform test_img[12] (use reshape(1,-1) for the image)\n",
    "# and approximate the original image.\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "pca = PCA(...)\n",
    "pca.fit(...)\n",
    "components = pca.transform(...)\n",
    "approximation = pca.inverse_transform(...)\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "original = test_img[12]\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(original.reshape(28,28)*255, cmap=plt.cm.gray)\n",
    "plt.title('Original Test Image \\n(784 components)')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.imshow(approximation.reshape(28,28)*255, cmap=plt.cm.gray)\n",
    "plt.title('Approximation \\n(40 components)')\n",
    "plt.xticks([]); plt.yticks([]);\n",
    "difference=rmse(original.reshape(28,28)*255,approximation.reshape(28,28)*255) \n",
    "print (\"The average difference between the two images is : \", np.round(difference,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __TASK__ : Decision tree with 95% explained variance\n",
    "Report the test accuracy of a decision tree model trained using components that account for 95% of the explained variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(12)\n",
    "# create a pipeline that instatnciates a Decision tree classifier\n",
    "# using components that explain 95% of the variance\n",
    "# and fit the train_img and train_lbl\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "pipeline = make_pipeline(..., ...)\n",
    "pipeline.fit(...,...)\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "preds = pipeline.predict(test_img)\n",
    "acc = accuracy_score(test_lbl, preds)\n",
    "print('Accuracy of Decision tree with 95% explained variance:', np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Homegrown KMeans Extension\n",
    "\n",
    "The  code below is a homegrown implementation of the KMeans algorithm. Study it carefully! \n",
    "\n",
    "For more background on this homegrown version of KMeans, please see this related [article](https://flothesof.github.io/k-means-numpy.html). This article presents lots of useful background information on the following:\n",
    "\n",
    "* Presents `broadcasting` a core piece of functionality underlying numpy \n",
    "* How to animate intermediate results of KMeans using a module called `JSAnimation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import animation\n",
    "import random\n",
    "from functools import reduce, partial\n",
    "from operator import add\n",
    "%matplotlib inline\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"good old class based solution\"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.means = [None for _ in range(k)]\n",
    "        \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"returns k centroids from the initial points\"\"\"\n",
    "        centroids = X.copy()\n",
    "        np.random.shuffle(centroids)\n",
    "        self.means = centroids[:self.k]\n",
    "        return \n",
    "\n",
    "    def fit(self, X, num_iters=10):\n",
    "        self.initialize_centroids(X)\n",
    "        for _ in range(num_iters):\n",
    "            # E-Step: assign each example to the nearest cluster\n",
    "            assignments = self.predict(X)  \n",
    "            #M-Step: for each cluster estimate new centroid based on cluster assigments\n",
    "            self.means = np.array([points[assignments==k].mean(axis=0) for k in range(self.k)])\n",
    "            \n",
    "\n",
    "    '''\n",
    "     # Broadcasting in the predict() method\n",
    "     \n",
    "     We will use numpy broadcasting to do cluster assignments in the predict() method. \n",
    "     One of the interesting things with numpy is that we can extend an array \n",
    "     by a new dimension using the np.newaxis() method. Here we go from a 2D array of centroids to \n",
    "     3D array. This allows us to substract this array from an existing point p, \n",
    "     due to the fact that numpy applies broadcasting rules to array of non-matching \n",
    "     sizes which allow for efficient operations\n",
    "     his is described in detail in the broadcasting section of the following document:\n",
    "     \n",
    "     http://nbviewer.ipython.org/url/www.astro.washington.edu/users/vanderplas/Astr599_2014/notebooks/11_EfficientNumpy.ipynb).\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        \"\"\"returns an array containing the index to the nearest centroid for each point\"\"\"\n",
    "        distances = np.sqrt(((X - self.means[:, np.newaxis])**2).sum(axis=2))\n",
    "        return np.argmin(distances, axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "def run_kmeans(points, k=3):\n",
    "    # Complete the code below to train a Kmeans model\n",
    "    # using k clsuters, fit points using 100 iterations (num_iters)\n",
    "    # and predict the cluster for each point\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    model = KMeans(...)\n",
    "    model.fit(..., ...)\n",
    "    assignments = model.predict(...) \n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "    print (\"the point points[75] is assigned to cluster\",assignments[75])\n",
    "    for x, y in model.means:\n",
    "        plt.plot(x, y, marker='X', markersize=20, color='Black')\n",
    "\n",
    "    for j, color in zip(range(k),\n",
    "                      ['r', 'g', 'b', 'm', 'c']):\n",
    "        cluster = [p\n",
    "                   for p, c in zip(points, assignments)\n",
    "                   if j == c]\n",
    "        xs, ys = zip(*cluster)\n",
    "        plt.scatter(xs, ys, color=color)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Generate some data\n",
    "np.random.seed(42)\n",
    "points = np.random.random((100,2))\n",
    "run_kmeans(points, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TASK: Extend the KMeans  implementation above with KMean++ \n",
    "\n",
    "How does the execution of the original KMeans code change when using KMeans++? Discuss. HINT: number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPlusPlus(KMeans):\n",
    "    \n",
    "    def _choose_next_center(self, X, distances):\n",
    "        self.probs = distances/distances.sum()\n",
    "        self.cumprobs = self.probs.cumsum()\n",
    "        random.seed(42)\n",
    "        r = random.random()\n",
    "        ind = np.where(self.cumprobs >= r)[0][0]\n",
    "        return([np.array(X[ind])])\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        random.seed(42)\n",
    "        self.means = np.array(random.sample(list(X), 1))\n",
    "        while len(self.means) < self.k:\n",
    "            # Complete the missing parts of the following code \n",
    "            # in order to calculate the minimum euclidian distance\n",
    "            # of each point from each center\n",
    "            #==================================================#\n",
    "            #               Your code starts here              #\n",
    "            # try the debugger!\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "            # HINT\n",
    "            distances = np.min(, axis=0)\n",
    "            self.means = np.append(..., self._choose_next_center(....., distances), axis = 0)\n",
    "            #==================================================#\n",
    "            #               Your code ends here                #\n",
    "            #               Please don't add code below here   #\n",
    "            #==================================================#        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpp = KPlusPlus(3)\n",
    "kpp.fit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "assignments = [kpp.predict(point) for point in points]\n",
    "\n",
    "for x, y in kpp.means:\n",
    "    plt.plot(x, y, marker='X', markersize=20, color='Black')\n",
    "\n",
    "for j, color in zip(range(3), ['r', 'g', 'b', 'm', 'c']):\n",
    "    cluster = [p for p, c in zip(points, assignments)if j == c]\n",
    "    xs, ys = zip(*cluster)\n",
    "    plt.scatter(xs, ys, color=color)\n",
    "    \n",
    "plt.show()\n",
    "print (\"the point points[50] is assigned to cluster\",assignments[50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional Task] Mickey Mouse case study: Plot cluster boundaries for first 10 iterations\n",
    "\n",
    "Generate a plausible  artificial \"mouse\" dataset depicted below (with the ground truth). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image2.png\" alt=\"Drawing\" style=\"width: =300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the homegrown Kmeans (KMeans++) plot the decision boundaries iteration by iteration for the first 10 iterations using 3 clusters. A plot like the following be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image3.png\" alt=\"Drawing\" style=\"width: =300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the missing parts of the following code \n",
    "# in order to create three clusters of points where:\n",
    "# the first cluster has a (-1,2) center, .3 std and random_state=42\n",
    "# the second cluster has a (.5,.5) center, .7 std and random_state=41\n",
    "# the third cluster has a (2,2) center, .3 std and random_state=40\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "x1, y1 = make_blobs(200, 2, centers = [[..]], cluster_std=.., ...)\n",
    "x2, y2 = make_blobs(200, 2, centers = [[..]], cluster_std=.., ...)\n",
    "x3, y3 = make_blobs(200, 2, centers = [[..]], cluster_std=.., ...)\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#\n",
    "y2 = np.full_like(y1, 1)\n",
    "y3 = np.full_like(y1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.append(np.append(x1, x2, axis=0), x3, axis=0)\n",
    "y = np.append(y1,(y2, y3))\n",
    "true_clusters =np.concatenate((np.repeat(0, 200), np.repeat(1, 200),np.repeat(2, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points[:,0],points[:,1],c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit homegrown KMeans++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below to train a KPlusPlus model\n",
    "# using 3 clsuters, fit points \n",
    "# and predict the cluster for each point\n",
    "#==================================================#\n",
    "#               Your code starts here              #\n",
    "#==================================================#\n",
    "model = KPlusPlus(...)\n",
    "model.fit(..., ...)\n",
    "assignments = model.predict(...)\n",
    "#==================================================#\n",
    "#               Your code ends here                #\n",
    "#               Please don't add code below here   #\n",
    "#==================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision surface\n",
    "x1_min, x1_max = points[:, 0].min() - .1, points[:, 0].max() + .1\n",
    "x2_min, x2_max = points[:, 1].min() - .1, points[:, 1].max() + .1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, .005), np.arange(x2_min, x2_max, .005))\n",
    "Z = np.array([kpp.predict(point) for point in np.array([xx1.ravel(), xx2.ravel()]).T])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3)\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "for x, y in kpp.means:\n",
    "    plt.plot(x, y, marker='X', markersize=20, color='Black')\n",
    "\n",
    "for j, color in zip(range(3), ['r', 'g', 'b', 'm', 'c']):\n",
    "    cluster = [p for p, c in zip(points, assignments)if j == c]\n",
    "    xs, ys = zip(*cluster)\n",
    "    plt.scatter(xs, ys)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completness score\n",
    "Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.\n",
    "\n",
    "In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:\n",
    "\n",
    "* homogeneity: each cluster contains only members of a single class.\n",
    "* completeness: all members of a given class are assigned to the same cluster.\n",
    "\n",
    "Homogeneity and completeness scores are formally given by:\n",
    "* $ h = 1 - \\frac{H(C|K)}{H(C)}$\n",
    "* $c = 1 - \\frac{H(K|C)}{H(K)}$\n",
    "\n",
    "where H(C|K) is the conditional entropy of the classes given the cluster assignments and is given by:  \n",
    "    $H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)$\n",
    "    \n",
    "and H(C) is the entropy of the classes and is given by:  \n",
    "$H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)$   \n",
    "\n",
    "\n",
    "with n the total number of samples, $n_c$ and $n_k$ the number of samples respectively belonging to class c and cluster k, and finally $n_c,k$ the number of samples from class c assigned to cluster k.\n",
    "\n",
    "The conditional entropy of clusters given class $H(K|C)$ and the entropy of clusters $H(K)$ are defined in a symmetric manner.\n",
    "\n",
    "* __Example__:  \n",
    "The following example shows how the completeness score is calculated for different clustering scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import completeness_score\n",
    "import scipy, pylab\n",
    "import matplotlib.pyplot as plt\n",
    "# example 1\n",
    "example_points= np.array([[1,0],[1,1],[1,2],[2,2],[3,1],[3,3]])\n",
    "labels_true, labels_pred = [0, 0, 1, 1,2,2], [1, 1, 2,2, 0, 0]\n",
    "fig, ((ax1, ax2)) = plt.subplots(1, 2)\n",
    "fig.suptitle(f\"Example 1: completeness_score({labels_true, labels_pred})= {completeness_score(labels_true, labels_pred)}\", fontsize=14)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "ax1.scatter(example_points[:,0],example_points[:,1],c=labels_true)\n",
    "ax1.set(title=\"Example 1 True clusters\")\n",
    "ax1.grid()\n",
    "ax2.scatter(example_points[:,0],example_points[:,1],c=labels_pred)\n",
    "ax2.grid()\n",
    "ax2.set(title=\"Example 1 predicted clusters\")\n",
    "plt.show()\n",
    "labels_true, labels_pred = [0, 0, 1, 1,2,2], [1, 1, 1,2, 0, 0]\n",
    "fig1, ((ax11, ax12)) = plt.subplots(1, 2)\n",
    "fig1.suptitle(f\"Example 2: completeness_score({labels_true, labels_pred})= {np.round(completeness_score(labels_true, labels_pred),3)}\", fontsize=14)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "ax11.scatter(example_points[:,0],example_points[:,1],c=labels_true)\n",
    "ax11.set(title=\"Example 2 True clusters\")\n",
    "ax11.grid()\n",
    "ax12.scatter(example_points[:,0],example_points[:,1],c=labels_pred)\n",
    "ax12.grid()\n",
    "ax12.set(title=\"Example 2 predicted clusters\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK__: Calculate the completeness score for the homegrown kmeans results obtained in section 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import completeness_score\n",
    "print(\"The completeness score is : \",np.round(completeness_score(labels_pred=assignments,labels_true =true_clusters),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette coefficient\n",
    "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n",
    "\n",
    "* a: The mean distance between each sample point and all other points in the same class.\n",
    "* b: The mean distance between each sample  point and all other points in the next nearest cluster.\n",
    "The Silhouette Coefficient s for a single sample is then given as:  \n",
    "$ s = \\frac{b - a}{max(a, b)} $  \n",
    "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\n",
    "* __Example__:  \n",
    "The following example shows how the silhouette score is calculated for different clustering scenarios. You can see that in the second case, when the points of the first cluster are closer to each other and far from the point in cluster 2 the silhouette score increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy, pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_points= np.array([[1,0],[1,1],[1,2],[2,2],[3,1],[3,3]])\n",
    "clusters= [1,1,2,2,3,3]\n",
    "# Create figure\n",
    "fig, ((ax1, ax2)) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.scatter(example_points[:,0],example_points[:,1],c=clusters)\n",
    "ax1.set(title=f\"Cluster example 1\\nclusters are not compact\\nsilhouette_score: {np.round(silhouette_score(example_points, clusters, metric='euclidean'),3)}\")\n",
    "ax1.grid()\n",
    "\n",
    "print(\"The silhouette score is : \",np.round(silhouette_score(example_points, clusters, metric='euclidean'),3))\n",
    "example_points= np.array([[1,0],[1.1,0.1],[1.2,0.2],[0.9,0],[1,0.2],[5,5]])\n",
    "clusters= [1,1,1,1,1,2]\n",
    "print(\"The silhouette score is : \",np.round(silhouette_score(example_points, clusters, metric='euclidean'),3))\n",
    "\n",
    "ax2.scatter(example_points[:,0],example_points[:,1],c=clusters)\n",
    "ax2.grid()\n",
    "ax2.set(title=f\"Cluster example 2\\nCompact clusters that are far apart\\nsilhouette_score: {np.round(silhouette_score(example_points, clusters, metric='euclidean'),3)}\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TASK__: Calculate the silhouette score for the homegrown kmeans results obtained in section 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(\"The silhouette score is : \",np.round(silhouette_score(points, assignments, metric='euclidean'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method: determine best K\n",
    "When fitting the Kmeans algorithm we chose to assign the different points into 3 cluster. Here we are using the elbow method in order to determine if there's a better number of clusters that we can choose using the k-means inertia graph. We will plot a line chart of the SSE (Sum of squared errors) for each value of k. If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "ks = range(2, 7)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans (using sklearn) instance with k clusters: model \n",
    "    # Fit model to samples\n",
    "    # and append the inertia to the list of inertias\n",
    "    #==================================================#\n",
    "    #               Your code starts here              #\n",
    "    #==================================================#\n",
    "    model = \n",
    "    model.fit(...)\n",
    "    inertias.append(...)\n",
    "    #==================================================#\n",
    "    #               Your code ends here                #\n",
    "    #               Please don't add code below here   #\n",
    "    #==================================================#\n",
    "    \n",
    "    \n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.title('Elbow Method for Identifying K Clusters')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
